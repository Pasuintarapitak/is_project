from pyexpat import model

from sklearn.metrics import classification_report, confusion_matrix
import streamlit as st

import random
import pandas as pd

from streamlit_option_menu import option_menu
from model_handler import predict  # ‡∏î‡∏∂‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
import tensorflow_datasets as tfds


st.set_page_config(page_title="My App", page_icon="üåü", layout="wide")
nsafe_allow_html=True

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Navbar ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô
selected = option_menu(
    menu_title=None,  
    options=["Machine Learning Documentation", "Neural Network Documentation"],  
    icons=["info-circle", "info-circle"],  
    menu_icon="cast",  
    default_index=0,  
    orientation="horizontal",
)


if selected == "Machine Learning Documentation":
    # st.title()
   
    #Header
    st.title(":orange[Cancer] Prediction Model  :sunglasses:")
    st.write("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á")

    #Dataset
    df = pd.read_csv("Dataset/cancer_dataset.csv")
    st.dataframe(df)
    st.markdown("***‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏≤‡∏Å ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏≤‡∏¢‡∏∏ ‡πÄ‡∏û‡∏® bmi ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏ß‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á***")
    st.markdown("***Raw Dateset : https://www.kaggle.com/datasets/rabieelkharoua/cancer-prediction-dataset/data***")
   
    #Feature
    st.subheader("Features Explanation:", divider="red")
    feature = pd.DataFrame(
    {
        "Features": ["Age", "Gender","BMI","GeneticRisk","PhysicalActivity",
                         "AlcoholIntake","CancerHistory","Diagnosis"],
        "Meaning": ["‡∏≠‡∏≤‡∏¢‡∏∏   ( ‡∏ä‡πà‡∏ß‡∏á 20 - 80 ‡∏õ‡∏µ )", 
                    "‡πÄ‡∏û‡∏® :  Male(‡∏ä‡∏≤‡∏¢)  ,  Female(‡∏´‡∏ç‡∏¥‡∏á)",
                    "‡∏Ñ‡πà‡∏≤‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏°‡∏ß‡∏•‡∏Å‡∏≤‡∏¢    (‡∏Å‡∏¥‡πÇ‡∏•‡∏Å‡∏£‡∏±‡∏°)",
                    "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏á‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡∏Å‡∏£‡∏£‡∏° ‡∏£‡∏∞‡∏î‡∏±‡∏ö: (  ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≥ ,‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏•‡∏≤‡∏á ,‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á  )",
                    "‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á  (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)",
                    "‡∏Å‡∏≤‡∏£‡∏î‡∏∑‡πà‡∏°‡πÅ‡∏≠‡∏•‡∏Å‡∏≠‡∏Æ‡∏≠‡∏•‡πå  (‡∏ï‡πà‡∏≠‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)",
                    "‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á",
                    "‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢ (Outputs)"],

    }
    )
    # st.dataframe(feature, height=300, width=600, use_container_width=True)
    feature = feature.reset_index(drop=True) 
    st.dataframe(feature,height=320, width=1200)


    #prepare data
    st.title(":red[Prepare] dataset")

    st.write("1. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á dataset ‡∏à‡∏≤‡∏Å ‡πÑ‡∏ü‡∏•‡πå .csv")
    code = '''
        data = pd. read_csv("/content/cancer_dataset.csv") 
    '''
    st.code(code, language="python")


    st.write("2. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NULL ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì")
    code = '''
        #Drop data which row has null
        data = data.dropna()
        
        #Convert data type to number for evaluation
        data['Gender'] = data['Gender'].map({'Female': 1, 'Male': 0})
        data['Smoking'] = data['Smoking'].map({'No': 0, 'Yes': 1})
        data['GeneticRisk'] = data['GeneticRisk'].map({'indicating Low': 0, 'indicating Medium': 1 , 'indicating High':2})
        data['CancerHistory'] = data['CancerHistory'].map({'No': 0, 'Yes': 1})
        data['Diagnosis'] = data['Diagnosis'].map({'Negative': 0, 'Positive': 1})
    '''
    st.code(code, language="python")

    st.write("4. ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    df = df.dropna()
    df['Gender'] = df['Gender'].map({'Female': 1, 'Male': 0})
    df['Smoking'] = df['Smoking'].map({'No': 0, 'Yes': 1})
    df['GeneticRisk'] = df['GeneticRisk'].map({'indicating Low': 0, 'indicating Medium': 1 , 'indicating High':2})
    df['CancerHistory'] = df['CancerHistory'].map({'No': 0, 'Yes': 1})
    df['Diagnosis'] = df['Diagnosis'].map({'Negative': 0, 'Positive': 1})
    st.dataframe(df)



    st.write("5. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å Feature ‡πÅ‡∏•‡∏∞ Target ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train sets ‡πÅ‡∏•‡∏∞ Test sets ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ")
    code = '''
        #Separate Features and Target
        X = data.drop('Diagnosis', axis=1)  # Features
        y = data['Diagnosis']  # Target

        # Separate Train sets (80 %) and Test sets (20 %) 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    '''
    st.code(code, language="python")


   


    
    #Train alogorithm
    st.title(":red[Traning] and :orange[Testing]")

    st.subheader("**Algorithm**üéØ")
    st.markdown("""
                1. Random Forest : ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ò‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å Decision Tree ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Ensemble Learning ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Overfitting ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                2. SVM (Support Vector Machine) : ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ò‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (Classification) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡πà‡∏≤ (Regression) ‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
              
    """)
    st.subheader("**Random Tree**üéØ",divider=True)
    st.write("1. ‡∏Å‡∏≤‡∏£ train Model ‡∏î‡πâ‡∏ß‡∏¢ Random Tree")
    code = '''
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Tree
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
    '''
    st.code(code, language="python")
    st.write("2. ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    
    code = '''
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
     
        y_pred = model.predict(X_test)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy
        accuracy = accuracy_score(y_test, y_pred)

        #  Classification Report
        report = (classification_report(y_test, y_pred))

        #  Confusion Matrix
        martirx = (confusion_matrix(y_test, y_pred))

    '''
    st.code(code, language="python")

    #SVM train
    st.subheader("SVM (Support Vector Machine)üéØ",divider=True)

    st.write("1. ‡∏Å‡∏≤‡∏£ train Model ‡∏î‡πâ‡∏ß‡∏¢ SVM")
    
    code = '''
        from sklearn.svm import SVC

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• SVM
        modelSVM = SVC(kernel='poly', probability=True)  
        modelSVM.fit(X_train, y_train)

    '''
    st.code(code, language="python")
    st.markdown("""
        *‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Normalization*
              
    """)
    
    st.write("2. ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    
    code = '''
        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    
        y_pred = model.predict(X_test)

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy
        accuracy = accuracy_score(y_test, y_pred)

        #  Classification Report
        report = (classification_report(y_test, y_pred))

        #  Confusion Matrix
        martirx = (confusion_matrix(y_test, y_pred))

    '''
    st.code(code, language="python")


   
    


elif selected == "Neural Network Documentation":


    st.title(":orange[Dog] Breed Prediction Model üê∂")
    st.write("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏™‡∏∏‡∏ô‡∏±‡∏Ç‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û")

    def load_dataset():
        dataset_name = "stanford_dogs"
        dataset, info = tfds.load(dataset_name, with_info=True, as_supervised=True)
        return dataset, info

    dataset, info = load_dataset()
    label_names = info.features["label"].int2str 

    train_data, test_data = dataset["train"], dataset["test"]
    sample_data = list(train_data.take(5))  
    #image
    cols = st.columns(5)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á columns 5 ‡∏≠‡∏±‡∏ô
    for col_idx, (image, label) in enumerate(sample_data):
        breed_name = label_names(label.numpy()) 
        with cols[col_idx]:  
            st.image(image.numpy(), caption=breed_name, width=180) 
    
    st.markdown("***‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏∏‡∏™‡∏∏‡∏ô‡∏±‡∏Ç 120 ‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏π‡πå ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û***")
    st.markdown("***Raw Dateset : https://www.tensorflow.org/datasets/catalog/stanford_dogs***")
    
    #prepare data
    st.title(":red[Prepare] dataset")

    st.write("1. ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á dataset  'stanford_dogs ' ‡∏à‡∏≤‡∏Å tensorflow ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å Train set ‡∏Å‡∏±‡∏ö Test set")
    code = '''
    def load_dataset():
        dataset_name = "stanford_dogs"
        dataset, info = tfds.load(dataset_name, with_info=True, as_supervised=True)
        return dataset, info
    dataset, info = load_dataset() 

    train_data, test_data = dataset["train"], dataset["test"]
    '''
    st.code(code, language="python")
    st.markdown("***standford_dogs ‡∏°‡∏µ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 20,580 ‡∏£‡∏π‡∏õ : Train 12,000 ‡∏£‡∏π‡∏õ + Test 8,580 ‡∏£‡∏π‡∏õ***")

    st.write("2. ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞ batch size")
    code = '''
        batch_size = 32
        image_size = (128, 128)  
    '''
    st.code(code, language="python")


    st.write("3. ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ô‡∏≥‡πÑ‡∏õ train")
    code = '''
        def preprocess(image, label):
            image = tf.cast(image, tf.float32) / 255.0  # Normalize
            image = tf.image.resize(image, image_size)  # Resize
            return image, label
    '''
    st.code(code, language="python")

    st.write("4. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train ‡πÅ‡∏•‡∏∞ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Test")
    code = '''
        # ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£Train
        train_data = (
            train_data
            .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)  # Preprocess ‡∏Å‡πà‡∏≠‡∏ô
            .shuffle(1000)
            .batch(batch_size)
            .prefetch(tf.data.AUTOTUNE)
        )

        # ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£Test
        test_data = (
            test_data
            .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
            .batch(batch_size)
            .prefetch(tf.data.AUTOTUNE)
        )

    '''
    st.code(code, language="python")

    #Train alogorithm
    st.title(":red[Traning] and :orange[Testing]")

    st.subheader("**Algorithm**üéØ")
    st.markdown("""
        - Convolutional Neural Network (CNN) : ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Neural Network ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏∂‡∏á ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏π‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÇ‡∏î‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å ‡∏Ç‡∏≠‡∏ö‡∏†‡∏≤‡∏û ‡∏û‡∏∑‡πâ‡∏ô‡∏ú‡∏¥‡∏ß ‡∏£‡∏π‡∏õ‡∏ó‡∏£‡∏á ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô‡∏†‡∏≤‡∏û
        ‡πÇ‡∏î‡∏¢‡∏°‡∏µ Convolutional Layers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö features ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û 
                      
    """)
    st.write("**CNN ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢**")
    st.markdown("""
  
        1. Convolutional Layer : ‡∏î‡∏∂‡∏á features ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Filter(Kernel) ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤
        2. Pooling Layer : ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
        3. Flatten Layer : ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ (vector)
        4. Fully Connected Layer : ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå
        5. Activation Function : ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ ‡πÄ‡∏ä‡πà‡∏ô ReLU ‡πÅ‡∏•‡∏∞ Softmax   
    """)
    st.markdown("""
        - MoblieNet : ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á CNN ‡∏ó‡∏µ‡πà Google ‡∏Ñ‡∏¥‡∏î‡∏Ñ‡πâ‡∏ô ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î model ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ö‡∏ô Moblie Devices              
    """)
    st.write("**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á MoblieNet**")
    st.markdown("""
        1. Depthwise Separable Convolution : ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å CNN ‡πÉ‡∏ä‡πâ Convolutional Layers ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå  
        2. Width Multiplier : ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á ‡πÇ‡∏î‡∏¢‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feature map
        3. Resolution Multiplier : ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î input ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß    
        """)
    st.subheader("**Training model**üéØ",divider=True)
    st.write("1. ‡πÇ‡∏´‡∏•‡∏î MoblieNetV2 ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ù‡∏∂‡∏Å")
    code = '''
        base_model = tf.keras.applications.MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights="imagenet")
        base_model.trainable = False
    '''
    st.code(code, language="python")

    st.write("2. train model ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ MoblieNetV2")
    code = '''
        #‡∏™‡∏£‡πâ‡∏≤‡∏á Model
        model = tf.keras.Sequential([
            base,
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(128, activation="relu"),
            tf.keras.layers.Dense(120, activation="softmax")
        ])

        model.compile(optimizer="adam",loss="sparse_categorical_crossentropy",metrics=["accuracy"]) #compile model
        #adam ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö learning rate ‡πÑ‡∏î‡πâ‡∏î‡∏µ

        class_weights = {i: 1.0 for i in range(120)} # 120 breeds
        early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        model.fit(train_data, epochs=10, validation_data=test_data, class_weight=class_weights, callbacks=[early_stopping])

        model.save('MobileNetV2_model.keras')  # savemodel
        return model
    '''
    st.code(code, language="python")

    st.write("Train function")
    code = '''
    def train_model():
        base = tf.keras.applications.MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights="imagenet")
        base.trainable = False

        #‡∏™‡∏£‡πâ‡∏≤‡∏á Model
        model = tf.keras.Sequential([
            base,
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Dense(128, activation="relu"),
            tf.keras.layers.Dense(120, activation="softmax")
        ])

        model.compile(optimizer="adam",loss="sparse_categorical_crossentropy",metrics=["accuracy"]) #compile model
        #adam ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö learning rate ‡πÑ‡∏î‡πâ‡∏î‡∏µ

        class_weights = {i: 1.0 for i in range(120)} # 120 breeds
        early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        model.fit(train_data, epochs=10, validation_data=test_data, class_weight=class_weights, callbacks=[early_stopping])

        model.save('MobileNetV2_model.keras')  # save model
        return model
    '''
    st.code(code, language="python")

    #Train alogorithm
    st.title(":red[Neural] Network :orange[App] ")

    st.write("1. ‡πÇ‡∏´‡∏•‡∏î model ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå .keras")
    code = '''
        def load_model():
            return tf.keras.models.load_model('models/MobileNetV2_model.keras')

        model = load_model()
    '''
    st.code(code, language="python")

    
    st.write("2. ‡πÇ‡∏´‡∏•‡∏î dataset ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    code = '''
        def get_label_map():
            _, info = tfds.load("stanford_dogs", with_info=True)
            return info.features['label'].int2str  # convert index to breed

        label_map = get_label_map()
    '''
    st.code(code, language="python")

    st.write("3. ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    code = '''
        def preprocess_image(image):
            image = image.convert("RGB")  
            image = image.resize((128, 128))  
            image = np.array(image) / 255.0  
            image = np.expand_dims(image, axis=0)  
            return image

    '''
    st.code(code, language="python")

    st.write("4. ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ó‡πå")
    code = '''
        uploaded_file = st.file_uploader("Upload your dog picture", type=["jpg", "jpeg", "png"])
        if uploaded_file:
            image = Image.open(uploaded_file)
            st.image(image, caption="üì∏ ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î", use_column_width=True)

            # remove bg
            image_no_bg = remove(image)
            # prepare for prediction
            processed_image = preprocess_image(image)
            predictions = model.predict(processed_image)

            
            predicted_class = np.argmax(predictions)  
            confidence = np.max(predictions)  #‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á model
            predicted_label = label_map(predicted_class)

            # Result
            st.subheader("üîç ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
            st.write(f"üê∂ ‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏™‡∏∏‡∏ô‡∏±‡∏Ç: **{predicted_label}**")
            st.write(f"‚úÖ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: **{confidence:.2%}**")

    '''
    st.code(code, language="python")